{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ElectraConfig, CamembertTokenizer, ElectraForCausalLM\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.electra.modeling_electra.ElectraForCausalLM'> is overwritten by shared decoder config: ElectraConfig {\n",
      "  \"_name_or_path\": \"/project/lt200324-optmul/pluem/model/huggingface_electra-small-25000-no-grad-small\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): ElectraForCausalLM(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(25005, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (generator_predictions): ElectraGeneratorPredictions(\n",
       "      (activation): GELUActivation()\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dense): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (generator_lm_head): Linear(in_features=128, out_features=25005, bias=True)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VisionEncoderDecoderModel.from_pretrained('openthaigpt/thai-trocr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionElectraModel(nn.Module):\n",
    "    def __init__(self, tokenizer, device='cpu'):\n",
    "        super(VisionElectraModel, self).__init__()\n",
    "\n",
    "        base_model = VisionEncoderDecoderModel.from_pretrained('openthaigpt/thai-trocr').to(device)\n",
    "        self.encoder = base_model.encoder\n",
    "        self.enc_to_dec_proj = base_model.enc_to_dec_proj\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        config = ElectraConfig.from_pretrained('openthaigpt/thai-trocr')\n",
    "        config.is_decoder = True\n",
    "        config.output_attentions   = True\n",
    "        config.add_cross_attention = True\n",
    "\n",
    "        self.decoder = VisionEncoderDecoderModel.from_pretrained('openthaigpt/thai-trocr').decoder.to(device)\n",
    "        # self.decoder = ElectraForCausalLM.from_pretrained('openthaigpt/thai-trocr', config=config)\n",
    "\n",
    "        self.decoder.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.decoder.config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.decoder.config.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.decoder.resize_token_embeddings(len(tokenizer.get_vocab()))\n",
    "        self.decoder.config.max_length = 20\n",
    "\n",
    "        self.vocab_size = self.decoder.config.vocab_size\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for name, param in self.decoder.electra.encoder.layer.named_parameters():\n",
    "            if 'crossattention' in name: param.requires_grad = True\n",
    "\n",
    "        self.loss_fnc = nn.CrossEntropyLoss(ignore_index=-100,).to(device)\n",
    "\n",
    "    def shift_tokens_right(self, input_ids, pad_token_id, bos_token_id):\n",
    "\n",
    "        # Create a shifted input with the bos_token at the start\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "        shifted_input_ids[:, 0] = bos_token_id  # Add bos_token at the start\n",
    "        \n",
    "        # Replace any trailing positions with pad_token_id\n",
    "        shifted_input_ids[shifted_input_ids == -100] = pad_token_id\n",
    "        \n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, pixel_values, labels, decoder_attention_mask):\n",
    "\n",
    "        with torch.no_grad(): # if VIT is freezeing!\n",
    "            patch_embeddings = self.encoder(pixel_values).last_hidden_state\n",
    "            projected_features = self.enc_to_dec_proj(patch_embeddings)\n",
    "        \n",
    "        decoder_input_ids = self.shift_tokens_right(\n",
    "                labels, self.decoder.config.pad_token_id, self.decoder.config.bos_token_id\n",
    "        )\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=projected_features,\n",
    "            encoder_attention_mask=None,\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fnc(logits.view(-1, self.vocab_size), labels.view(-1))\n",
    "\n",
    "        return outputs, loss\n",
    "    \n",
    "    def predict(self, pixel_values, max_length=20, device='cuda'):\n",
    "        \n",
    "        self.eval() \n",
    "        with torch.no_grad():\n",
    "\n",
    "            patch_embeddings = self.encoder(pixel_values).last_hidden_state\n",
    "            projected_features = self.enc_to_dec_proj(patch_embeddings)\n",
    "            \n",
    "            input_ids = torch.tensor([[self.decoder.config.bos_token_id]]).to(device)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                \n",
    "                outputs = self.decoder(\n",
    "                    input_ids=input_ids,\n",
    "                    encoder_hidden_states=projected_features,\n",
    "                    encoder_attention_mask=None,\n",
    "                    use_cache=True  # Speeds up decoding by caching past key values !!!\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits[:, -1, :]                           # last generated token [-1]\n",
    "                next_token_id = torch.argmax(logits, dim=-1).unsqueeze(0)   # select token with the highest prob\n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "                if next_token_id.item() == self.decoder.config.eos_token_id:\n",
    "                    break # Stop if EOS token is generated\n",
    "                \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"/mnt/e/ALPR-CEPJ1-Experiments-logs/projects/best-w/ex4-e149l0.3560c0.2044w0.2971.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.electra.modeling_electra.ElectraForCausalLM'> is overwritten by shared decoder config: ElectraConfig {\n",
      "  \"_name_or_path\": \"/project/lt200324-optmul/pluem/model/huggingface_electra-small-25000-no-grad-small\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "You are using a model of type vision-encoder-decoder to instantiate a model of type electra. This is not supported for all configurations of models and can yield errors.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.electra.modeling_electra.ElectraForCausalLM'> is overwritten by shared decoder config: ElectraConfig {\n",
      "  \"_name_or_path\": \"/project/lt200324-optmul/pluem/model/huggingface_electra-small-25000-no-grad-small\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "/tmp/ipykernel_18166/1502923128.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/mnt/e/ALPR-CEPJ1-Experiments-logs/projects/best-w/ex4-e149l0.3560c0.2044w0.2971.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"openthaigpt/thai-trocr\")\n",
    "model = VisionElectraModel(tokenizer=tokenizer, device='cpu')\n",
    "model.load_state_dict(torch.load(\"/mnt/e/ALPR-CEPJ1-Experiments-logs/projects/best-w/ex4-e149l0.3560c0.2044w0.2971.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.46.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.46.2\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from transformers==4.46.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from requests->transformers==4.46.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thanasak/miniconda3/envs/plate-recognizer/lib/python3.10/site-packages (from requests->transformers==4.46.2) (2024.8.30)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "Successfully installed transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.46.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_values = feature_extractor(cropped_character_images[1].convert('RGB'), return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# std = mean= [0.5, 0.5, 0.5]\n",
    "# plt.imshow(pixel_values.squeeze(dim=0).permute(1, 2, 0) * torch.tensor(std) + torch.tensor(mean))\n",
    "\n",
    "# predict_ids = model.predict(pixel_values, device='cpu')\n",
    "# predict_words = tokenizer.batch_decode(predict_ids, skip_special_tokens=True)\n",
    "# predict_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plate-recognizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
