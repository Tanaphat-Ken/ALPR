/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
2025-10-07 04:15:38,938 - INFO - train_trocr - Loading records from data/8000/8000.csv
2025-10-07 04:15:40,326 - INFO - train.data_utils - Loaded 8172 records (require_validate=True, drop_missing=True)
2025-10-07 04:15:40,326 - INFO - train_trocr - Creating stratified splits (train=0.70, val=0.20, test=0.10)
2025-10-07 04:15:40,337 - INFO - train.data_utils - Split 'train': 5589 samples
2025-10-07 04:15:40,337 - INFO - train.data_utils - Split 'val': 1536 samples
2025-10-07 04:15:40,337 - INFO - train.data_utils - Split 'test': 1047 samples
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
2025-10-07 04:15:43,526 - INFO - train_trocr - Added 77 province tokens to tokenizer
2025-10-07 04:15:43,527 - INFO - train_trocr - Loading openthaigpt/thai-trocr and applying state_dict from models/weights/charactor_reader.pth
/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py:780: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(source_path, map_location="cpu")
2025-10-07 04:15:45,557 - INFO - train_trocr - Vocab size mismatch: state_dict=25005, current_model=25005, required=25082
2025-10-07 04:15:45,645 - INFO - train_trocr - Resizing model decoder embeddings from 25005 to 25082 tokens after partial state_dict load
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2025-10-07 04:15:45,683 - INFO - train_trocr - Copying 25005 original embeddings to resized model
2025-10-07 04:15:45,700 - INFO - train_trocr - Resized decoder embeddings to 25082 tokens
/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py:1230: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `VisionSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = VisionSeq2SeqTrainer(
2025-10-07 04:15:45,830 - INFO - train_trocr - Starting training: 5589 train samples, 1536 val samples
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 5}.
  0%|          | 0/1750 [00:00<?, ?it/s]2025-10-07 04:15:54,651 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
  0%|          | 1/1750 [00:37<18:07:49, 37.32s/it]2025-10-07 04:16:23,342 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 2/1750 [00:52<11:51:56, 24.44s/it]2025-10-07 04:16:38,760 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 3/1750 [01:12<10:48:30, 22.27s/it]2025-10-07 04:16:58,459 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 4/1750 [01:31<10:11:22, 21.01s/it]2025-10-07 04:17:17,531 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 5/1750 [01:50<9:50:55, 20.32s/it] 2025-10-07 04:17:36,624 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 6/1750 [02:07<9:17:59, 19.20s/it]2025-10-07 04:17:53,644 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 7/1750 [02:24<8:59:01, 18.56s/it]2025-10-07 04:18:10,878 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 8/1750 [02:40<8:30:36, 17.59s/it]2025-10-07 04:18:26,394 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 9/1750 [02:59<8:40:33, 17.94s/it]2025-10-07 04:18:45,110 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 10/1750 [03:19<8:59:58, 18.62s/it]2025-10-07 04:19:05,251 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 11/1750 [03:38<9:02:02, 18.70s/it]2025-10-07 04:19:24,140 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 12/1750 [03:57<9:12:07, 19.06s/it]2025-10-07 04:19:44,020 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 13/1750 [04:17<9:12:59, 19.10s/it]2025-10-07 04:20:03,216 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 14/1750 [04:36<9:16:10, 19.22s/it]2025-10-07 04:20:22,717 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 15/1750 [04:53<8:51:26, 18.38s/it]2025-10-07 04:20:39,140 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 16/1750 [05:12<9:02:53, 18.78s/it]2025-10-07 04:20:58,873 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 17/1750 [05:32<9:07:22, 18.95s/it]2025-10-07 04:21:18,206 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 18/1750 [05:52<9:19:21, 19.38s/it]2025-10-07 04:21:38,591 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 19/1750 [06:12<9:21:13, 19.45s/it]2025-10-07 04:21:58,206 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 20/1750 [06:31<9:19:51, 19.42s/it]                                                     1%|          | 20/1750 [06:31<9:19:51, 19.42s/it]2025-10-07 04:22:17,547 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 21/1750 [06:49<9:09:37, 19.07s/it]2025-10-07 04:22:35,811 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|▏         | 22/1750 [07:10<9:21:56, 19.51s/it]2025-10-07 04:22:56,346 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|▏         | 23/1750 [07:27<8:58:29, 18.71s/it]2025-10-07 04:23:13,180 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|▏         | 24/1750 [07:46<9:06:34, 19.00s/it]2025-10-07 04:23:32,861 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|▏         | 25/1750 [08:06<9:12:24, 19.21s/it]2025-10-07 04:23:52,575 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|▏         | 26/1750 [08:24<8:56:59, 18.69s/it]2025-10-07 04:24:10,039 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 27/1750 [08:38<8:17:48, 17.34s/it]2025-10-07 04:24:24,216 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 28/1750 [08:51<7:41:27, 16.08s/it]2025-10-07 04:24:37,363 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 29/1750 [09:04<7:13:11, 15.10s/it]2025-10-07 04:24:50,188 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 30/1750 [09:29<8:40:35, 18.16s/it]2025-10-07 04:25:15,481 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 31/1750 [09:45<8:25:29, 17.64s/it]2025-10-07 04:25:31,919 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 32/1750 [10:07<8:55:32, 18.70s/it]2025-10-07 04:25:53,103 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 33/1750 [10:24<8:39:59, 18.17s/it]2025-10-07 04:26:10,039 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 34/1750 [10:41<8:33:48, 17.97s/it]2025-10-07 04:26:27,511 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 35/1750 [11:00<8:45:28, 18.38s/it]2025-10-07 04:26:46,872 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 36/1750 [11:21<9:05:51, 19.11s/it]2025-10-07 04:27:07,669 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 37/1750 [11:38<8:48:31, 18.51s/it]2025-10-07 04:27:24,789 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 38/1750 [12:00<9:18:06, 19.56s/it]2025-10-07 04:27:46,795 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 39/1750 [12:18<8:58:29, 18.88s/it]2025-10-07 04:28:04,101 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 40/1750 [12:35<8:49:04, 18.56s/it]                                                     2%|▏         | 40/1750 [12:35<8:49:04, 18.56s/it]2025-10-07 04:28:21,922 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 41/1750 [12:52<8:35:47, 18.11s/it]2025-10-07 04:28:38,965 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 42/1750 [13:16<9:22:41, 19.77s/it]2025-10-07 04:29:02,601 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  2%|▏         | 43/1750 [13:36<9:20:08, 19.69s/it]2025-10-07 04:29:22,108 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 44/1750 [13:54<9:06:08, 19.21s/it]2025-10-07 04:29:40,194 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 45/1750 [14:12<8:54:46, 18.82s/it]2025-10-07 04:29:58,106 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 46/1750 [14:31<8:56:35, 18.89s/it]2025-10-07 04:30:17,176 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 47/1750 [14:48<8:43:40, 18.45s/it]2025-10-07 04:30:34,589 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 48/1750 [15:07<8:45:11, 18.51s/it]2025-10-07 04:30:53,253 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 49/1750 [15:27<9:00:23, 19.06s/it]2025-10-07 04:31:13,591 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 50/1750 [15:44<8:44:50, 18.52s/it]2025-10-07 04:31:30,861 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 51/1750 [16:02<8:40:32, 18.38s/it]2025-10-07 04:31:48,916 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  3%|▎         | 52/1750 [16:22<8:50:03, 18.73s/it]2025-10-07 04:32:08,456 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
slurmstepd: error: *** JOB 30513 ON prism-1 CANCELLED AT 2025-10-07T04:32:26 ***
