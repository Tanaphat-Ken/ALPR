2025-10-06 23:40:05,081 - INFO - train_trocr - Loading records from data/8000/8000.csv
2025-10-06 23:40:06,456 - INFO - train.data_utils - Loaded 8172 records (require_validate=True, drop_missing=True)
2025-10-06 23:40:06,456 - INFO - train_trocr - Creating stratified splits (train=0.70, val=0.20, test=0.10)
2025-10-06 23:40:06,467 - INFO - train.data_utils - Split 'train': 5589 samples
2025-10-06 23:40:06,467 - INFO - train.data_utils - Split 'val': 1536 samples
2025-10-06 23:40:06,467 - INFO - train.data_utils - Split 'test': 1047 samples
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
2025-10-06 23:40:09,463 - INFO - train_trocr - Added 77 province tokens to tokenizer
2025-10-06 23:40:11,147 - INFO - train_trocr - Resizing fresh model decoder embeddings from 25005 to 25082 tokens (tokenizer len: 25081, max_id: 25081)
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
2025-10-06 23:40:11,557 - WARNING - train_trocr - Vocab size mismatch: tokenizer=25081, model=25082
2025-10-06 23:40:11,716 - INFO - train_trocr - Resized decoder embeddings to 25081 tokens
/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py:1153: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `VisionSeq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = VisionSeq2SeqTrainer(
2025-10-06 23:40:11,815 - INFO - train_trocr - Starting training: 5589 train samples, 1536 val samples
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 5}.
  0%|          | 0/702 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
2025-10-06 23:40:16,098 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
  0%|          | 1/702 [01:00<11:50:55, 60.85s/it]2025-10-06 23:41:12,875 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 2/702 [01:50<10:31:00, 54.09s/it]2025-10-06 23:42:02,227 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  0%|          | 3/702 [02:39<10:05:44, 52.00s/it]2025-10-06 23:42:51,738 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 4/702 [03:29<9:52:35, 50.94s/it] 2025-10-06 23:43:41,052 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 5/702 [04:18<9:46:45, 50.51s/it]2025-10-06 23:44:30,804 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 6/702 [05:08<9:43:18, 50.29s/it]2025-10-06 23:45:20,662 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
  1%|          | 7/702 [05:57<9:38:46, 49.97s/it]2025-10-06 23:46:09,961 - INFO - train_trocr - VisionSeq2SeqTrainer model kwargs (loss): ['pixel_values', 'labels']
Traceback (most recent call last):
  File "/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py", line 1193, in <module>
    main()
  File "/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py", line 1165, in main
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/tsomboo/ALPR/plate_recognizer/train/train_trocr.py", line 444, in compute_loss
    outputs = model(**model_kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py", line 553, in forward
    decoder_outputs = self.decoder(
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 1506, in forward
    outputs = self.roberta(
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 858, in forward
    embedding_output = self.embeddings(
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py", line 111, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 190, in forward
    return F.embedding(
  File "/home/tsomboo/ALPR/plate_recognizer/.venv_linux/lib/python3.10/site-packages/torch/nn/functional.py", line 2551, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
  1%|          | 7/702 [06:13<10:17:31, 53.31s/it]
2025-10-06 23:46:40,432 - INFO - train.data_utils - Loaded 8172 records (require_validate=True, drop_missing=True)
2025-10-06 23:46:40,443 - INFO - train.data_utils - Split 'train': 5589 samples
2025-10-06 23:46:40,443 - INFO - train.data_utils - Split 'val': 1121 samples
2025-10-06 23:46:40,443 - INFO - train.data_utils - Split 'test': 1462 samples
2025-10-06 23:46:40,443 - INFO - eval_trocr - Selected 1462 samples from test split
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
slurmstepd: error: *** JOB 30459 ON prism-1 CANCELLED AT 2025-10-06T23:46:42 ***
